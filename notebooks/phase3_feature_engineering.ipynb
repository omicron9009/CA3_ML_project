{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔧 Road Accident Analysis - Phase 3: Feature Engineering & Preprocessing\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 Phase 3 Objectives:\n",
        "1. **Feature Creation**: Time-based features, age groups, risk scores\n",
        "2. **Handling Missing Values**: Imputation strategies\n",
        "3. **Encoding Categorical Variables**: Label, One-Hot, Target encoding\n",
        "4. **Feature Scaling**: StandardScaler, MinMaxScaler\n",
        "5. **Handling Class Imbalance**: SMOTE, class weights\n",
        "6. **Feature Selection**: Correlation analysis, feature importance\n",
        "7. **Train-Test Split**: Stratified splitting\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Libraries imported successfully!\n",
            "Pandas Version: 2.3.2\n",
            "NumPy Version: 1.26.4\n",
            "Scikit-learn & Imbalanced-learn ready!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Import Required Libraries\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from scipy import stats\n",
        "\n",
        "# Preprocessing libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Configure display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print('✓ Libraries imported successfully!')\n",
        "print(f'Pandas Version: {pd.__version__}')\n",
        "print(f'NumPy Version: {np.__version__}')\n",
        "print(f'Scikit-learn & Imbalanced-learn ready!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "DATASET LOADED SUCCESSFULLY\n",
            "================================================================================\n",
            "Dataset Shape: 3000 rows × 22 columns\n",
            "================================================================================\n",
            "\n",
            "✓ Working copy created: df_processed\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Load Dataset and Recreate Feature Lists\n",
        "# ============================================================================\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('dataset\\\\accident_prediction_india.csv')\n",
        "\n",
        "print('='*80)\n",
        "print('DATASET LOADED SUCCESSFULLY')\n",
        "print('='*80)\n",
        "print(f'Dataset Shape: {df.shape[0]} rows × {df.shape[1]} columns')\n",
        "print('='*80)\n",
        "\n",
        "# Create a copy for preprocessing\n",
        "df_processed = df.copy()\n",
        "\n",
        "print('\\n✓ Working copy created: df_processed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 🆕 FEATURE CREATION\n",
        "Creating new features from existing data\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FEATURE ENGINEERING: TIME-BASED FEATURES\n",
            "================================================================================\n",
            "\n",
            "✓ Time-based features created:\n",
            "  • Hour (0-23)\n",
            "  • Time_Period (Morning/Afternoon/Evening/Night)\n",
            "  • Is_Weekend (0/1)\n",
            "  • Season (Winter/Spring/Summer/Autumn)\n",
            "\n",
            "  Time_Period Distribution:\n",
            "Time_Period\n",
            "Night        1039\n",
            "Morning       894\n",
            "Afternoon     597\n",
            "Evening       470\n",
            "Name: count, dtype: int64\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Feature Engineering - Time-Based Features\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('FEATURE ENGINEERING: TIME-BASED FEATURES')\n",
        "print('='*80)\n",
        "\n",
        "# 1. Extract Hour from Time of Day (if in HH:MM format)\n",
        "def extract_hour(time_str):\n",
        "    try:\n",
        "        if pd.isna(time_str):\n",
        "            return np.nan\n",
        "        if ':' in str(time_str):\n",
        "            return int(str(time_str).split(':')[0])\n",
        "        return np.nan\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "df_processed['Hour'] = df_processed['Time of Day'].apply(extract_hour)\n",
        "\n",
        "# 2. Create Time Period categories\n",
        "def categorize_time_period(hour):\n",
        "    if pd.isna(hour):\n",
        "        return 'Unknown'\n",
        "    elif 5 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'Afternoon'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "df_processed['Time_Period'] = df_processed['Hour'].apply(categorize_time_period)\n",
        "\n",
        "# 3. Weekend flag\n",
        "df_processed['Is_Weekend'] = df_processed['Day of Week'].apply(\n",
        "    lambda x: 1 if x in ['Saturday', 'Sunday'] else 0\n",
        ")\n",
        "\n",
        "# 4. Season from Month\n",
        "def get_season(month):\n",
        "    if month in ['December', 'January', 'February']:\n",
        "        return 'Winter'\n",
        "    elif month in ['March', 'April', 'May']:\n",
        "        return 'Spring'\n",
        "    elif month in ['June', 'July', 'August']:\n",
        "        return 'Summer'\n",
        "    else:\n",
        "        return 'Autumn'\n",
        "\n",
        "df_processed['Season'] = df_processed['Month'].apply(get_season)\n",
        "\n",
        "print('\\n✓ Time-based features created:')\n",
        "print('  • Hour (0-23)')\n",
        "print('  • Time_Period (Morning/Afternoon/Evening/Night)')\n",
        "print('  • Is_Weekend (0/1)')\n",
        "print('  • Season (Winter/Spring/Summer/Autumn)')\n",
        "\n",
        "print(f'\\n  Time_Period Distribution:\\n{df_processed[\"Time_Period\"].value_counts()}')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FEATURE ENGINEERING: AGE GROUPS & DEMOGRAPHICS\n",
            "================================================================================\n",
            "\n",
            "✓ Demographic features created:\n",
            "  • Driver_Age_Group (Young/Adult/Middle-Aged/Senior)\n",
            "  • Speed_Category (Low/Medium/High)\n",
            "\n",
            "  Driver Age Group Distribution:\n",
            "Driver_Age_Group\n",
            "Adult          1069\n",
            "Middle-Aged     886\n",
            "Senior          628\n",
            "Young           417\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  Speed Category Distribution:\n",
            "Speed_Category\n",
            "High_Speed      1359\n",
            "Medium_Speed     943\n",
            "Low_Speed        698\n",
            "Name: count, dtype: int64\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Feature Engineering - Age Groups & Demographics\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('FEATURE ENGINEERING: AGE GROUPS & DEMOGRAPHICS')\n",
        "print('='*80)\n",
        "\n",
        "# 1. Driver Age Groups\n",
        "def categorize_age(age):\n",
        "    if pd.isna(age):\n",
        "        return 'Unknown'\n",
        "    elif age < 25:\n",
        "        return 'Young'\n",
        "    elif 25 <= age < 45:\n",
        "        return 'Adult'\n",
        "    elif 45 <= age < 60:\n",
        "        return 'Middle-Aged'\n",
        "    else:\n",
        "        return 'Senior'\n",
        "\n",
        "df_processed['Driver_Age_Group'] = df_processed['Driver Age'].apply(categorize_age)\n",
        "\n",
        "# 2. Speed Category\n",
        "def categorize_speed(speed):\n",
        "    if pd.isna(speed):\n",
        "        return 'Unknown'\n",
        "    elif speed < 50:\n",
        "        return 'Low_Speed'\n",
        "    elif 50 <= speed < 80:\n",
        "        return 'Medium_Speed'\n",
        "    else:\n",
        "        return 'High_Speed'\n",
        "\n",
        "df_processed['Speed_Category'] = df_processed['Speed Limit (km/h)'].apply(categorize_speed)\n",
        "\n",
        "print('\\n✓ Demographic features created:')\n",
        "print('  • Driver_Age_Group (Young/Adult/Middle-Aged/Senior)')\n",
        "print('  • Speed_Category (Low/Medium/High)')\n",
        "\n",
        "print(f'\\n  Driver Age Group Distribution:\\n{df_processed[\"Driver_Age_Group\"].value_counts()}')\n",
        "print(f'\\n  Speed Category Distribution:\\n{df_processed[\"Speed_Category\"].value_counts()}')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FEATURE ENGINEERING: RISK INDICATORS & INTERACTIONS\n",
            "================================================================================\n",
            "\n",
            "✓ Risk indicator features created:\n",
            "  • High_Risk_Weather (0/1)\n",
            "  • Poor_Visibility (0/1)\n",
            "  • High_Risk_Road (0/1)\n",
            "  • High_Casualty_Count (0/1)\n",
            "  • Has_Fatality (0/1)\n",
            "  • Multi_Vehicle_Accident (0/1)\n",
            "  • Risk_Score (0-5 composite score)\n",
            "\n",
            "  Risk Score Distribution:\n",
            "Risk_Score\n",
            "0      91\n",
            "1     460\n",
            "2    1016\n",
            "3     939\n",
            "4     413\n",
            "5      81\n",
            "Name: count, dtype: int64\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Feature Engineering - Risk Indicators & Interaction Features\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('FEATURE ENGINEERING: RISK INDICATORS & INTERACTIONS')\n",
        "print('='*80)\n",
        "\n",
        "# 1. High Risk Weather Flag\n",
        "high_risk_weather = ['Stormy', 'Foggy', 'Hazy']\n",
        "df_processed['High_Risk_Weather'] = df_processed['Weather Conditions'].apply(\n",
        "    lambda x: 1 if x in high_risk_weather else 0\n",
        ")\n",
        "\n",
        "# 2. Poor Visibility Flag (Dark + Bad Weather)\n",
        "df_processed['Poor_Visibility'] = ((df_processed['Lighting Conditions'] == 'Dark') & \n",
        "                                    (df_processed['High_Risk_Weather'] == 1)).astype(int)\n",
        "\n",
        "# 3. High Risk Road Condition\n",
        "df_processed['High_Risk_Road'] = df_processed['Road Condition'].apply(\n",
        "    lambda x: 1 if x in ['Wet', 'Under Construction', 'Damaged'] else 0\n",
        ")\n",
        "\n",
        "# 4. Fatal Risk Flag (multiple casualties or fatalities)\n",
        "df_processed['High_Casualty_Count'] = (df_processed['Number of Casualties'] >= 5).astype(int)\n",
        "df_processed['Has_Fatality'] = (df_processed['Number of Fatalities'] > 0).astype(int)\n",
        "\n",
        "# 5. Multiple Vehicle Accident\n",
        "df_processed['Multi_Vehicle_Accident'] = (df_processed['Number of Vehicles Involved'] > 2).astype(int)\n",
        "\n",
        "# 6. Composite Risk Score (0-5 scale)\n",
        "df_processed['Risk_Score'] = (\n",
        "    df_processed['High_Risk_Weather'] +\n",
        "    df_processed['Poor_Visibility'] +\n",
        "    df_processed['High_Risk_Road'] +\n",
        "    (df_processed['Alcohol Involvement'] == 'Yes').astype(int) +\n",
        "    (df_processed['Speed_Category'] == 'High_Speed').astype(int)\n",
        ")\n",
        "\n",
        "print('\\n✓ Risk indicator features created:')\n",
        "print('  • High_Risk_Weather (0/1)')\n",
        "print('  • Poor_Visibility (0/1)')\n",
        "print('  • High_Risk_Road (0/1)')\n",
        "print('  • High_Casualty_Count (0/1)')\n",
        "print('  • Has_Fatality (0/1)')\n",
        "print('  • Multi_Vehicle_Accident (0/1)')\n",
        "print('  • Risk_Score (0-5 composite score)')\n",
        "\n",
        "print(f'\\n  Risk Score Distribution:\\n{df_processed[\"Risk_Score\"].value_counts().sort_index()}')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 🔧 HANDLING MISSING VALUES\n",
        "Imputation strategies for missing data\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "HANDLING MISSING VALUES\n",
            "================================================================================\n",
            "\n",
            "📊 Missing Values Before Imputation:\n",
            "  • Driver License Status               :   975 (32.50%)\n",
            "  • Traffic Control Presence            :   716 (23.87%)\n",
            "\n",
            "📊 Missing Values After Imputation:\n",
            "  ✓ All missing values handled!\n",
            "\n",
            "✓ Imputation Strategy Applied:\n",
            "  • Categorical: Filled with \"Unknown\"\n",
            "  • Numerical: Filled with median values\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Handling Missing Values\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('HANDLING MISSING VALUES')\n",
        "print('='*80)\n",
        "\n",
        "# Check missing values before\n",
        "missing_before = df_processed.isnull().sum()\n",
        "missing_before = missing_before[missing_before > 0].sort_values(ascending=False)\n",
        "\n",
        "print('\\n📊 Missing Values Before Imputation:')\n",
        "if len(missing_before) > 0:\n",
        "    for col, count in missing_before.items():\n",
        "        percentage = (count / len(df_processed)) * 100\n",
        "        print(f'  • {col:<35} : {count:>5} ({percentage:>5.2f}%)')\n",
        "else:\n",
        "    print('  ✓ No missing values found!')\n",
        "\n",
        "# Strategy: Fill missing values\n",
        "# 1. Driver License Status - Fill with 'Unknown'\n",
        "if 'Driver License Status' in df_processed.columns:\n",
        "    df_processed['Driver License Status'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# 2. Traffic Control Presence - Fill with 'Unknown'\n",
        "if 'Traffic Control Presence' in df_processed.columns:\n",
        "    df_processed['Traffic Control Presence'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# 3. City Name - Already has 'Unknown' values\n",
        "if 'City Name' in df_processed.columns:\n",
        "    df_processed['City Name'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# 4. Numerical features - Fill with median\n",
        "numerical_cols = ['Speed Limit (km/h)', 'Driver Age', 'Hour']\n",
        "for col in numerical_cols:\n",
        "    if col in df_processed.columns:\n",
        "        df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
        "\n",
        "# Check missing values after\n",
        "missing_after = df_processed.isnull().sum()\n",
        "missing_after = missing_after[missing_after > 0].sort_values(ascending=False)\n",
        "\n",
        "print('\\n📊 Missing Values After Imputation:')\n",
        "if len(missing_after) > 0:\n",
        "    for col, count in missing_after.items():\n",
        "        percentage = (count / len(df_processed)) * 100\n",
        "        print(f'  • {col:<35} : {count:>5} ({percentage:>5.2f}%)')\n",
        "else:\n",
        "    print('  ✓ All missing values handled!')\n",
        "\n",
        "print('\\n✓ Imputation Strategy Applied:')\n",
        "print('  • Categorical: Filled with \"Unknown\"')\n",
        "print('  • Numerical: Filled with median values')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 🔤 ENCODING CATEGORICAL VARIABLES\n",
        "Converting categorical features to numerical format\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ENCODING: TARGET VARIABLE (ACCIDENT SEVERITY)\n",
            "================================================================================\n",
            "\n",
            "✓ Target Variable Encoded:\n",
            "  • Minor    → 0\n",
            "  • Serious  → 1\n",
            "  • Fatal    → 2\n",
            "\n",
            "Encoded Distribution:\n",
            "Severity_Encoded\n",
            "0    1034\n",
            "1     981\n",
            "2     985\n",
            "Name: count, dtype: int64\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Label Encoding for Target Variable\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('ENCODING: TARGET VARIABLE (ACCIDENT SEVERITY)')\n",
        "print('='*80)\n",
        "\n",
        "# Label encode target variable (ordinal encoding)\n",
        "severity_mapping = {'Minor': 0, 'Serious': 1, 'Fatal': 2}\n",
        "df_processed['Severity_Encoded'] = df_processed['Accident Severity'].map(severity_mapping)\n",
        "\n",
        "print('\\n✓ Target Variable Encoded:')\n",
        "print('  • Minor    → 0')\n",
        "print('  • Serious  → 1')\n",
        "print('  • Fatal    → 2')\n",
        "\n",
        "print(f'\\nEncoded Distribution:')\n",
        "print(df_processed['Severity_Encoded'].value_counts().sort_index())\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ENCODING: ORDINAL CATEGORICAL FEATURES\n",
            "================================================================================\n",
            "\n",
            "✓ Speed_Category encoded:\n",
            "  • Low_Speed       → 0\n",
            "  • Medium_Speed    → 1\n",
            "  • High_Speed      → 2\n",
            "  • Unknown         → -1\n",
            "\n",
            "✓ Driver_Age_Group encoded:\n",
            "  • Young           → 0\n",
            "  • Adult           → 1\n",
            "  • Middle-Aged     → 2\n",
            "  • Senior          → 3\n",
            "  • Unknown         → -1\n",
            "\n",
            "✓ Time_Period encoded:\n",
            "  • Morning         → 0\n",
            "  • Afternoon       → 1\n",
            "  • Evening         → 2\n",
            "  • Night           → 3\n",
            "  • Unknown         → -1\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Label Encoding for Ordinal Categorical Features\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('ENCODING: ORDINAL CATEGORICAL FEATURES')\n",
        "print('='*80)\n",
        "\n",
        "# Define ordinal features with order\n",
        "ordinal_mappings = {\n",
        "    'Speed_Category': {'Low_Speed': 0, 'Medium_Speed': 1, 'High_Speed': 2, 'Unknown': -1},\n",
        "    'Driver_Age_Group': {'Young': 0, 'Adult': 1, 'Middle-Aged': 2, 'Senior': 3, 'Unknown': -1},\n",
        "    'Time_Period': {'Morning': 0, 'Afternoon': 1, 'Evening': 2, 'Night': 3, 'Unknown': -1}\n",
        "}\n",
        "\n",
        "for col, mapping in ordinal_mappings.items():\n",
        "    if col in df_processed.columns:\n",
        "        df_processed[f'{col}_Encoded'] = df_processed[col].map(mapping)\n",
        "        print(f'\\n✓ {col} encoded:')\n",
        "        for key, val in mapping.items():\n",
        "            print(f'  • {key:<15} → {val}')\n",
        "\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ENCODING: BINARY CATEGORICAL FEATURES\n",
            "================================================================================\n",
            "\n",
            "✓ Alcohol Involvement encoded:\n",
            "  • Yes        → 1\n",
            "  • No         → 0\n",
            "\n",
            "✓ Driver Gender encoded:\n",
            "  • Male       → 0\n",
            "  • Female     → 1\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Binary Encoding for Binary Categorical Features\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('ENCODING: BINARY CATEGORICAL FEATURES')\n",
        "print('='*80)\n",
        "\n",
        "# Binary features\n",
        "binary_mappings = {\n",
        "    'Alcohol Involvement': {'Yes': 1, 'No': 0},\n",
        "    'Driver Gender': {'Male': 0, 'Female': 1}\n",
        "}\n",
        "\n",
        "for col, mapping in binary_mappings.items():\n",
        "    if col in df_processed.columns:\n",
        "        df_processed[f'{col}_Encoded'] = df_processed[col].map(mapping)\n",
        "        print(f'\\n✓ {col} encoded:')\n",
        "        for key, val in mapping.items():\n",
        "            print(f'  • {key:<10} → {val}')\n",
        "\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ENCODING: ONE-HOT ENCODING FOR NOMINAL FEATURES\n",
            "================================================================================\n",
            "\n",
            "📋 Features to be one-hot encoded: 8\n",
            "  • Weather Conditions (5 categories)\n",
            "  • Road Type (4 categories)\n",
            "  • Road Condition (4 categories)\n",
            "  • Lighting Conditions (4 categories)\n",
            "  • Vehicle Type Involved (7 categories)\n",
            "  • Day of Week (7 categories)\n",
            "  • Season (4 categories)\n",
            "  • Accident Location Details (4 categories)\n",
            "\n",
            "✓ One-hot encoding complete!\n",
            "  Original features: 41\n",
            "  Encoded features: 64\n",
            "  New columns added: 23\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: One-Hot Encoding for Nominal Categorical Features\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('ENCODING: ONE-HOT ENCODING FOR NOMINAL FEATURES')\n",
        "print('='*80)\n",
        "\n",
        "# Select nominal features for one-hot encoding (exclude high cardinality)\n",
        "nominal_features = [\n",
        "    'Weather Conditions',\n",
        "    'Road Type',\n",
        "    'Road Condition',\n",
        "    'Lighting Conditions',\n",
        "    'Vehicle Type Involved',\n",
        "    'Day of Week',\n",
        "    'Season',\n",
        "    'Accident Location Details'\n",
        "]\n",
        "\n",
        "# Filter existing features\n",
        "nominal_features = [col for col in nominal_features if col in df_processed.columns]\n",
        "\n",
        "print(f'\\n📋 Features to be one-hot encoded: {len(nominal_features)}')\n",
        "for feat in nominal_features:\n",
        "    print(f'  • {feat} ({df_processed[feat].nunique()} categories)')\n",
        "\n",
        "# Perform one-hot encoding\n",
        "df_encoded = pd.get_dummies(df_processed, columns=nominal_features, prefix=nominal_features, drop_first=True)\n",
        "\n",
        "print(f'\\n✓ One-hot encoding complete!')\n",
        "print(f'  Original features: {len(df_processed.columns)}')\n",
        "print(f'  Encoded features: {len(df_encoded.columns)}')\n",
        "print(f'  New columns added: {len(df_encoded.columns) - len(df_processed.columns)}')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 📐 FEATURE SCALING\n",
        "Normalizing numerical features for model training\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FEATURE SELECTION FOR MODELING\n",
            "================================================================================\n",
            "\n",
            "✓ Features selected for modeling: 51\n",
            "  Excluded features: 13\n",
            "\n",
            "📊 Numerical features to be scaled: 7\n",
            "  • Speed Limit (km/h)\n",
            "  • Driver Age\n",
            "  • Number of Vehicles Involved\n",
            "  • Number of Casualties\n",
            "  • Number of Fatalities\n",
            "  • Hour\n",
            "  • Risk_Score\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 11: Feature Selection & Preparation for Scaling\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('FEATURE SELECTION FOR MODELING')\n",
        "print('='*80)\n",
        "\n",
        "# Exclude features not needed for modeling\n",
        "exclude_cols = [\n",
        "    'State Name',  # High cardinality\n",
        "    'City Name',  # High cardinality (71% unknown)\n",
        "    'Time of Day',  # Already extracted Hour\n",
        "    'Month',  # Converted to Season\n",
        "    'Year',  # Optional - can keep for temporal analysis\n",
        "    'Accident Severity',  # Original target (use encoded version)\n",
        "    'Driver_Age_Group',  # Already encoded\n",
        "    'Speed_Category',  # Already encoded\n",
        "    'Time_Period',  # Already encoded\n",
        "    'Alcohol Involvement',  # Already encoded\n",
        "    'Driver Gender',  # Already encoded\n",
        "    'Traffic Control Presence',  # 23.87% missing, low importance\n",
        "    'Driver License Status'  # 32.5% missing, low importance\n",
        "]\n",
        "\n",
        "# Create modeling dataframe\n",
        "df_model = df_encoded.copy()\n",
        "\n",
        "# Drop excluded columns if they exist\n",
        "cols_to_drop = [col for col in exclude_cols if col in df_model.columns]\n",
        "df_model = df_model.drop(columns=cols_to_drop)\n",
        "\n",
        "print(f'\\n✓ Features selected for modeling: {len(df_model.columns)}')\n",
        "print(f'  Excluded features: {len(cols_to_drop)}')\n",
        "\n",
        "# Identify numerical features that need scaling\n",
        "numerical_features_to_scale = [\n",
        "    'Speed Limit (km/h)',\n",
        "    'Driver Age',\n",
        "    'Number of Vehicles Involved',\n",
        "    'Number of Casualties',\n",
        "    'Number of Fatalities',\n",
        "    'Hour',\n",
        "    'Risk_Score'\n",
        "]\n",
        "\n",
        "numerical_features_to_scale = [col for col in numerical_features_to_scale if col in df_model.columns]\n",
        "\n",
        "print(f'\\n📊 Numerical features to be scaled: {len(numerical_features_to_scale)}')\n",
        "for feat in numerical_features_to_scale:\n",
        "    print(f'  • {feat}')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TRAIN-TEST SPLIT\n",
            "================================================================================\n",
            "\n",
            "✓ Data split complete:\n",
            "  Training set: 2400 samples (80.0%)\n",
            "  Test set: 600 samples (20.0%)\n",
            "  Number of features: 48\n",
            "\n",
            "📊 Class Distribution in Training Set:\n",
            "  • Minor      :   827 (34.46%)\n",
            "  • Serious    :   785 (32.71%)\n",
            "  • Fatal      :   788 (32.83%)\n",
            "\n",
            "📊 Class Distribution in Test Set:\n",
            "  • Minor      :   207 (34.50%)\n",
            "  • Serious    :   196 (32.67%)\n",
            "  • Fatal      :   197 (32.83%)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 12: Train-Test Split (Before Scaling)\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('TRAIN-TEST SPLIT')\n",
        "print('='*80)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_model.drop(columns=['Severity_Encoded', 'Number of Casualties', 'Number of Fatalities'])\n",
        "y_classification = df_model['Severity_Encoded']\n",
        "y_casualties = df_model['Number of Casualties']\n",
        "y_fatalities = df_model['Number of Fatalities']\n",
        "\n",
        "# Stratified split for classification\n",
        "X_train, X_test, y_train_class, y_test_class = train_test_split(\n",
        "    X, y_classification, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_classification\n",
        ")\n",
        "\n",
        "# Same split for regression targets\n",
        "_, _, y_train_casualties, y_test_casualties = train_test_split(\n",
        "    X, y_casualties, \n",
        "    test_size=0.2, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "_, _, y_train_fatalities, y_test_fatalities = train_test_split(\n",
        "    X, y_fatalities, \n",
        "    test_size=0.2, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f'\\n✓ Data split complete:')\n",
        "print(f'  Training set: {X_train.shape[0]} samples ({(X_train.shape[0]/len(X))*100:.1f}%)')\n",
        "print(f'  Test set: {X_test.shape[0]} samples ({(X_test.shape[0]/len(X))*100:.1f}%)')\n",
        "print(f'  Number of features: {X_train.shape[1]}')\n",
        "\n",
        "print(f'\\n📊 Class Distribution in Training Set:')\n",
        "train_dist = y_train_class.value_counts().sort_index()\n",
        "for severity, count in train_dist.items():\n",
        "    severity_name = {0: 'Minor', 1: 'Serious', 2: 'Fatal'}[severity]\n",
        "    print(f'  • {severity_name:<10} : {count:>5} ({(count/len(y_train_class))*100:>5.2f}%)')\n",
        "\n",
        "print(f'\\n📊 Class Distribution in Test Set:')\n",
        "test_dist = y_test_class.value_counts().sort_index()\n",
        "for severity, count in test_dist.items():\n",
        "    severity_name = {0: 'Minor', 1: 'Serious', 2: 'Fatal'}[severity]\n",
        "    print(f'  • {severity_name:<10} : {count:>5} ({(count/len(y_test_class))*100:>5.2f}%)')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FEATURE SCALING: STANDARDSCALER\n",
            "================================================================================\n",
            "\n",
            "✓ StandardScaler applied to 5 features\n",
            "  Formula: z = (x - μ) / σ\n",
            "  Where: μ = mean, σ = standard deviation\n",
            "\n",
            "📊 Scaling Statistics (Training Set):\n",
            "  • Speed Limit (km/h)             | Mean:    74.99 | Std:    26.75\n",
            "  • Driver Age                     | Mean:    44.16 | Std:    15.30\n",
            "  • Number of Vehicles Involved    | Mean:     2.99 | Std:     1.43\n",
            "  • Hour                           | Mean:    11.36 | Std:     6.96\n",
            "  • Risk_Score                     | Mean:     2.46 | Std:     1.08\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 13: Feature Scaling - StandardScaler\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('FEATURE SCALING: STANDARDSCALER')\n",
        "print('='*80)\n",
        "\n",
        "# Initialize scaler\n",
        "scaler_standard = StandardScaler()\n",
        "\n",
        "# Identify numerical columns in X_train\n",
        "numerical_cols_in_X = [col for col in numerical_features_to_scale if col in X_train.columns]\n",
        "\n",
        "# Fit on training data and transform both train and test\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[numerical_cols_in_X] = scaler_standard.fit_transform(X_train[numerical_cols_in_X])\n",
        "X_test_scaled[numerical_cols_in_X] = scaler_standard.transform(X_test[numerical_cols_in_X])\n",
        "\n",
        "print(f'\\n✓ StandardScaler applied to {len(numerical_cols_in_X)} features')\n",
        "print('  Formula: z = (x - μ) / σ')\n",
        "print('  Where: μ = mean, σ = standard deviation')\n",
        "\n",
        "print(f'\\n📊 Scaling Statistics (Training Set):')\n",
        "for col in numerical_cols_in_X[:5]:  # Show first 5\n",
        "    mean_val = X_train[col].mean()\n",
        "    std_val = X_train[col].std()\n",
        "    print(f'  • {col:<30} | Mean: {mean_val:>8.2f} | Std: {std_val:>8.2f}')\n",
        "\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FEATURE SCALING: MINMAXSCALER (Alternative for Neural Networks)\n",
            "================================================================================\n",
            "\n",
            "✓ MinMaxScaler applied to 5 features\n",
            "  Formula: x_scaled = (x - x_min) / (x_max - x_min)\n",
            "  Range: [0, 1]\n",
            "\n",
            "📊 Scaling Range (Training Set):\n",
            "  • Speed Limit (km/h)             | Min:    30.00 | Max:   120.00\n",
            "  • Driver Age                     | Min:    18.00 | Max:    70.00\n",
            "  • Number of Vehicles Involved    | Min:     1.00 | Max:     5.00\n",
            "  • Hour                           | Min:     0.00 | Max:    23.00\n",
            "  • Risk_Score                     | Min:     0.00 | Max:     5.00\n",
            "\n",
            "💡 Note: Use X_train_scaled/X_test_scaled for tree-based models\n",
            "         Use X_train_normalized/X_test_normalized for neural networks\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 14: Feature Scaling - MinMaxScaler (Alternative)\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('FEATURE SCALING: MINMAXSCALER (Alternative for Neural Networks)')\n",
        "print('='*80)\n",
        "\n",
        "# Initialize MinMaxScaler\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "# Create normalized versions\n",
        "X_train_normalized = X_train.copy()\n",
        "X_test_normalized = X_test.copy()\n",
        "\n",
        "X_train_normalized[numerical_cols_in_X] = scaler_minmax.fit_transform(X_train[numerical_cols_in_X])\n",
        "X_test_normalized[numerical_cols_in_X] = scaler_minmax.transform(X_test[numerical_cols_in_X])\n",
        "\n",
        "print(f'\\n✓ MinMaxScaler applied to {len(numerical_cols_in_X)} features')\n",
        "print('  Formula: x_scaled = (x - x_min) / (x_max - x_min)')\n",
        "print('  Range: [0, 1]')\n",
        "\n",
        "print(f'\\n📊 Scaling Range (Training Set):')\n",
        "for col in numerical_cols_in_X[:5]:  # Show first 5\n",
        "    min_val = X_train[col].min()\n",
        "    max_val = X_train[col].max()\n",
        "    print(f'  • {col:<30} | Min: {min_val:>8.2f} | Max: {max_val:>8.2f}')\n",
        "\n",
        "print('\\n💡 Note: Use X_train_scaled/X_test_scaled for tree-based models')\n",
        "print('         Use X_train_normalized/X_test_normalized for neural networks')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ⚖️ HANDLING CLASS IMBALANCE\n",
        "Techniques to balance the target variable distribution\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CLASS IMBALANCE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "📊 Current Class Distribution (Training Set):\n",
            "  • Minor      :   827 (34.46%)\n",
            "  • Serious    :   785 (32.71%)\n",
            "  • Fatal      :   788 (32.83%)\n",
            "\n",
            "📈 Imbalance Ratio: 1.05:1\n",
            "  ✓ Classes are well balanced\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 15: Class Imbalance Analysis\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('CLASS IMBALANCE ANALYSIS')\n",
        "print('='*80)\n",
        "\n",
        "# Calculate class distribution\n",
        "class_counts = y_train_class.value_counts().sort_index()\n",
        "class_percentages = (class_counts / len(y_train_class)) * 100\n",
        "\n",
        "print('\\n📊 Current Class Distribution (Training Set):')\n",
        "for severity, count in class_counts.items():\n",
        "    severity_name = {0: 'Minor', 1: 'Serious', 2: 'Fatal'}[severity]\n",
        "    percentage = class_percentages[severity]\n",
        "    print(f'  • {severity_name:<10} : {count:>5} ({percentage:>5.2f}%)')\n",
        "\n",
        "# Calculate imbalance ratio\n",
        "imbalance_ratio = class_counts.max() / class_counts.min()\n",
        "print(f'\\n📈 Imbalance Ratio: {imbalance_ratio:.2f}:1')\n",
        "\n",
        "if imbalance_ratio < 1.5:\n",
        "    print('  ✓ Classes are well balanced')\n",
        "elif imbalance_ratio < 3:\n",
        "    print('  ⚠ Slight imbalance - consider using class weights')\n",
        "else:\n",
        "    print('  ⚠ Significant imbalance - SMOTE recommended')\n",
        "\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "APPLYING SMOTE (Synthetic Minority Over-sampling Technique)\n",
            "================================================================================\n",
            "\n",
            "✓ SMOTE applied successfully!\n",
            "  Before SMOTE: 2400 samples\n",
            "  After SMOTE: 2481 samples\n",
            "  New samples created: 81\n",
            "\n",
            "📊 Class Distribution After SMOTE:\n",
            "  • Minor      :   827 (33.33%)\n",
            "  • Serious    :   827 (33.33%)\n",
            "  • Fatal      :   827 (33.33%)\n",
            "\n",
            "💡 Note: Use X_train_smote & y_train_smote for model training with balanced data\n",
            "         Keep X_test_scaled & y_test_class unchanged for evaluation\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 16: SMOTE - Synthetic Minority Oversampling\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('APPLYING SMOTE (Synthetic Minority Over-sampling Technique)')\n",
        "print('='*80)\n",
        "\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "\n",
        "# Apply SMOTE to training data\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train_class)\n",
        "\n",
        "print('\\n✓ SMOTE applied successfully!')\n",
        "print(f'  Before SMOTE: {X_train_scaled.shape[0]} samples')\n",
        "print(f'  After SMOTE: {X_train_smote.shape[0]} samples')\n",
        "print(f'  New samples created: {X_train_smote.shape[0] - X_train_scaled.shape[0]}')\n",
        "\n",
        "print(f'\\n📊 Class Distribution After SMOTE:')\n",
        "smote_dist = y_train_smote.value_counts().sort_index()\n",
        "for severity, count in smote_dist.items():\n",
        "    severity_name = {0: 'Minor', 1: 'Serious', 2: 'Fatal'}[severity]\n",
        "    print(f'  • {severity_name:<10} : {count:>5} ({(count/len(y_train_smote))*100:>5.2f}%)')\n",
        "\n",
        "print('\\n💡 Note: Use X_train_smote & y_train_smote for model training with balanced data')\n",
        "print('         Keep X_test_scaled & y_test_class unchanged for evaluation')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "COMPUTING CLASS WEIGHTS (Alternative to SMOTE)\n",
            "================================================================================\n",
            "\n",
            "✓ Class weights computed:\n",
            "  • Minor      : 0.9674\n",
            "  • Serious    : 1.0191\n",
            "  • Fatal      : 1.0152\n",
            "\n",
            "💡 Usage: Pass class_weight parameter to models\n",
            "  Example: RandomForestClassifier(class_weight=class_weight_dict)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 17: Compute Class Weights (Alternative to SMOTE)\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('COMPUTING CLASS WEIGHTS (Alternative to SMOTE)')\n",
        "print('='*80)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train_class),\n",
        "    y=y_train_class\n",
        ")\n",
        "\n",
        "# Create class weight dictionary\n",
        "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "print('\\n✓ Class weights computed:')\n",
        "for severity, weight in class_weight_dict.items():\n",
        "    severity_name = {0: 'Minor', 1: 'Serious', 2: 'Fatal'}[severity]\n",
        "    print(f'  • {severity_name:<10} : {weight:.4f}')\n",
        "\n",
        "print('\\n💡 Usage: Pass class_weight parameter to models')\n",
        "print('  Example: RandomForestClassifier(class_weight=class_weight_dict)')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 💾 SAVE PREPROCESSED DATA\n",
        "Export processed datasets for model training\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SAVING PREPROCESSED DATASETS\n",
            "================================================================================\n",
            "\n",
            "✓ All datasets saved successfully!\n",
            "\n",
            "📁 Files created:\n",
            "  Classification Data:\n",
            "    • X_train_scaled.csv, X_test_scaled.csv\n",
            "    • X_train_normalized.csv, X_test_normalized.csv\n",
            "    • X_train_smote.csv, y_train_smote.csv\n",
            "    • y_train_classification.csv, y_test_classification.csv\n",
            "  Regression Data:\n",
            "    • y_train_casualties.csv, y_test_casualties.csv\n",
            "    • y_train_fatalities.csv, y_test_fatalities.csv\n",
            "  Model Objects:\n",
            "    • class_weights.pkl\n",
            "    • scaler_standard.pkl, scaler_minmax.pkl\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 18: Save Preprocessed Datasets\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('SAVING PREPROCESSED DATASETS')\n",
        "print('='*80)\n",
        "\n",
        "# Save as CSV\n",
        "X_train_scaled.to_csv('X_train_scaled.csv', index=False)\n",
        "X_test_scaled.to_csv('X_test_scaled.csv', index=False)\n",
        "X_train_normalized.to_csv('X_train_normalized.csv', index=False)\n",
        "X_test_normalized.to_csv('X_test_normalized.csv', index=False)\n",
        "\n",
        "# Save SMOTE data\n",
        "pd.DataFrame(X_train_smote, columns=X_train_scaled.columns).to_csv('X_train_smote.csv', index=False)\n",
        "pd.DataFrame(y_train_smote, columns=['Severity_Encoded']).to_csv('y_train_smote.csv', index=False)\n",
        "\n",
        "# Save target variables\n",
        "y_train_class.to_csv('y_train_classification.csv', index=False)\n",
        "y_test_class.to_csv('y_test_classification.csv', index=False)\n",
        "y_train_casualties.to_csv('y_train_casualties.csv', index=False)\n",
        "y_test_casualties.to_csv('y_test_casualties.csv', index=False)\n",
        "y_train_fatalities.to_csv('y_train_fatalities.csv', index=False)\n",
        "y_test_fatalities.to_csv('y_test_fatalities.csv', index=False)\n",
        "\n",
        "# Save class weights\n",
        "import pickle\n",
        "with open('class_weights.pkl', 'wb') as f:\n",
        "    pickle.dump(class_weight_dict, f)\n",
        "\n",
        "# Save scalers\n",
        "with open('scaler_standard.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_standard, f)\n",
        "with open('scaler_minmax.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_minmax, f)\n",
        "\n",
        "print('\\n✓ All datasets saved successfully!')\n",
        "print('\\n📁 Files created:')\n",
        "print('  Classification Data:')\n",
        "print('    • X_train_scaled.csv, X_test_scaled.csv')\n",
        "print('    • X_train_normalized.csv, X_test_normalized.csv')\n",
        "print('    • X_train_smote.csv, y_train_smote.csv')\n",
        "print('    • y_train_classification.csv, y_test_classification.csv')\n",
        "print('  Regression Data:')\n",
        "print('    • y_train_casualties.csv, y_test_casualties.csv')\n",
        "print('    • y_train_fatalities.csv, y_test_fatalities.csv')\n",
        "print('  Model Objects:')\n",
        "print('    • class_weights.pkl')\n",
        "print('    • scaler_standard.pkl, scaler_minmax.pkl')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 3 SUMMARY REPORT: FEATURE ENGINEERING & PREPROCESSING\n",
            "================================================================================\n",
            "\n",
            "🎯 TASKS COMPLETED:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. FEATURE ENGINEERING:\n",
            "  ✓ Time-based features: Hour, Time_Period, Is_Weekend, Season\n",
            "  ✓ Demographic features: Driver_Age_Group, Speed_Category\n",
            "  ✓ Risk indicators: High_Risk_Weather, Poor_Visibility, Risk_Score\n",
            "  ✓ Interaction features: Multi_Vehicle_Accident, Has_Fatality\n",
            "\n",
            "2. MISSING VALUE TREATMENT:\n",
            "  ✓ Categorical: Filled with \"Unknown\"\n",
            "  ✓ Numerical: Filled with median values\n",
            "  ✓ Zero missing values after imputation\n",
            "\n",
            "3. FEATURE ENCODING:\n",
            "  ✓ Target variable: Label encoded (0-2)\n",
            "  ✓ Ordinal features: Label encoded with order\n",
            "  ✓ Binary features: Binary encoded (0/1)\n",
            "  ✓ Nominal features: One-hot encoded (8 features)\n",
            "\n",
            "4. FEATURE SCALING:\n",
            "  ✓ StandardScaler applied to 5 numerical features\n",
            "  ✓ MinMaxScaler created for neural network models\n",
            "\n",
            "5. TRAIN-TEST SPLIT:\n",
            "  ✓ Training samples: 2400 (80%)\n",
            "  ✓ Test samples: 600 (20%)\n",
            "  ✓ Total features: 48\n",
            "  ✓ Stratified split maintained class distribution\n",
            "\n",
            "6. CLASS IMBALANCE HANDLING:\n",
            "  ✓ SMOTE applied: 2400 → 2481 samples\n",
            "  ✓ Class weights computed for weighted learning\n",
            "  ✓ Balanced dataset ready for training\n",
            "\n",
            "7. DATA EXPORT:\n",
            "  ✓ 12 CSV files saved\n",
            "  ✓ 3 pickle files saved (scalers & weights)\n",
            "  ✓ Ready for Phase 4: Model Training\n",
            "\n",
            "================================================================================\n",
            "📊 FINAL DATASET STATISTICS:\n",
            "--------------------------------------------------------------------------------\n",
            "Original Dataset: 3000 rows × 22 columns\n",
            "Processed Dataset: 2481 training samples × 48 features\n",
            "Feature Engineering Added: 19 new features\n",
            "One-Hot Encoding Added: 23 dummy variables\n",
            "\n",
            "✓ PHASE 3: FEATURE ENGINEERING & PREPROCESSING COMPLETED!\n",
            "================================================================================\n",
            "\n",
            "🚀 Ready for Phase 4: Model Development & Training\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 19: Phase 3 Summary Report\n",
        "# ============================================================================\n",
        "\n",
        "print('='*80)\n",
        "print('PHASE 3 SUMMARY REPORT: FEATURE ENGINEERING & PREPROCESSING')\n",
        "print('='*80)\n",
        "\n",
        "print('\\n🎯 TASKS COMPLETED:')\n",
        "print('-' * 80)\n",
        "\n",
        "print('\\n1. FEATURE ENGINEERING:')\n",
        "print('  ✓ Time-based features: Hour, Time_Period, Is_Weekend, Season')\n",
        "print('  ✓ Demographic features: Driver_Age_Group, Speed_Category')\n",
        "print('  ✓ Risk indicators: High_Risk_Weather, Poor_Visibility, Risk_Score')\n",
        "print('  ✓ Interaction features: Multi_Vehicle_Accident, Has_Fatality')\n",
        "\n",
        "print('\\n2. MISSING VALUE TREATMENT:')\n",
        "print('  ✓ Categorical: Filled with \"Unknown\"')\n",
        "print('  ✓ Numerical: Filled with median values')\n",
        "print('  ✓ Zero missing values after imputation')\n",
        "\n",
        "print('\\n3. FEATURE ENCODING:')\n",
        "print('  ✓ Target variable: Label encoded (0-2)')\n",
        "print('  ✓ Ordinal features: Label encoded with order')\n",
        "print('  ✓ Binary features: Binary encoded (0/1)')\n",
        "print(f'  ✓ Nominal features: One-hot encoded ({len(nominal_features)} features)')\n",
        "\n",
        "print('\\n4. FEATURE SCALING:')\n",
        "print(f'  ✓ StandardScaler applied to {len(numerical_cols_in_X)} numerical features')\n",
        "print('  ✓ MinMaxScaler created for neural network models')\n",
        "\n",
        "print('\\n5. TRAIN-TEST SPLIT:')\n",
        "print(f'  ✓ Training samples: {X_train.shape[0]} (80%)')\n",
        "print(f'  ✓ Test samples: {X_test.shape[0]} (20%)')\n",
        "print(f'  ✓ Total features: {X_train.shape[1]}')\n",
        "print('  ✓ Stratified split maintained class distribution')\n",
        "\n",
        "print('\\n6. CLASS IMBALANCE HANDLING:')\n",
        "print(f'  ✓ SMOTE applied: {X_train_scaled.shape[0]} → {X_train_smote.shape[0]} samples')\n",
        "print('  ✓ Class weights computed for weighted learning')\n",
        "print('  ✓ Balanced dataset ready for training')\n",
        "\n",
        "print('\\n7. DATA EXPORT:')\n",
        "print('  ✓ 12 CSV files saved')\n",
        "print('  ✓ 3 pickle files saved (scalers & weights)')\n",
        "print('  ✓ Ready for Phase 4: Model Training')\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('📊 FINAL DATASET STATISTICS:')\n",
        "print('-' * 80)\n",
        "print(f'Original Dataset: {df.shape[0]} rows × {df.shape[1]} columns')\n",
        "print(f'Processed Dataset: {X_train_smote.shape[0]} training samples × {X_train_smote.shape[1]} features')\n",
        "print(f'Feature Engineering Added: {len(df_processed.columns) - len(df.columns)} new features')\n",
        "print(f'One-Hot Encoding Added: {len(df_encoded.columns) - len(df_processed.columns)} dummy variables')\n",
        "\n",
        "print('\\n✓ PHASE 3: FEATURE ENGINEERING & PREPROCESSING COMPLETED!')\n",
        "print('='*80)\n",
        "print('\\n🚀 Ready for Phase 4: Model Development & Training')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
